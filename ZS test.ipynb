{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVR\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "%matplotlib inline"
   ]
  },
   {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_file.csv')\n",
    "test_df = pd.read_csv('test_file.csv')\n",
    "test_id = test['IDLink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tr3CMgRv1N</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wc81vGp8qZ</td>\n",
       "      <td>A Look at the Health of the Chinese Economy</td>\n",
       "      <td>Tim Haywood, investment director business-unit...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2008-09-20 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>-0.156386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zNGH03CrZH</td>\n",
       "      <td>Nouriel Roubini: Global Economy Not Back to 2008</td>\n",
       "      <td>Nouriel Roubini, NYU professor and chairman at...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.425210</td>\n",
       "      <td>0.139754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3sM1H0W8ts</td>\n",
       "      <td>Finland GDP Expands In Q4</td>\n",
       "      <td>Finland's economy expanded marginally in the t...</td>\n",
       "      <td>RTT News</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:06:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wUbnxgvqaZ</td>\n",
       "      <td>Tourism, govt spending buoys Thai economy in J...</td>\n",
       "      <td>Tourism and public spending continued to boost...</td>\n",
       "      <td>The Nation - Thailand&amp;#39;s English news</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:11:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IDLink                                              Title  \\\n",
       "0  Tr3CMgRv1N   Obama Lays Wreath at Arlington National Cemetery   \n",
       "1  Wc81vGp8qZ        A Look at the Health of the Chinese Economy   \n",
       "2  zNGH03CrZH   Nouriel Roubini: Global Economy Not Back to 2008   \n",
       "3  3sM1H0W8ts                          Finland GDP Expands In Q4   \n",
       "4  wUbnxgvqaZ  Tourism, govt spending buoys Thai economy in J...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...   \n",
       "1  Tim Haywood, investment director business-unit...   \n",
       "2  Nouriel Roubini, NYU professor and chairman at...   \n",
       "3  Finland's economy expanded marginally in the t...   \n",
       "4  Tourism and public spending continued to boost...   \n",
       "\n",
       "                                     Source    Topic          PublishDate  \\\n",
       "0                                 USA TODAY    obama  2002-04-02 00:00:00   \n",
       "1                                 Bloomberg  economy  2008-09-20 00:00:00   \n",
       "2                                 Bloomberg  economy  2012-01-28 00:00:00   \n",
       "3                                  RTT News  economy  2015-03-01 00:06:00   \n",
       "4  The Nation - Thailand&#39;s English news  economy  2015-03-01 00:11:00   \n",
       "\n",
       "   Facebook  GooglePlus  LinkedIn  SentimentTitle  SentimentHeadline  \n",
       "0        -1          -1        -1        0.000000          -0.053300  \n",
       "1        -1          -1        -1        0.208333          -0.156386  \n",
       "2        -1          -1        -1       -0.425210           0.139754  \n",
       "3        -1          -1        -1        0.000000           0.026064  \n",
       "4        -1          -1        -1        0.000000           0.141084  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55932 entries, 0 to 55931\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   IDLink             55932 non-null  object \n",
      " 1   Title              55932 non-null  object \n",
      " 2   Headline           55932 non-null  object \n",
      " 3   Source             55757 non-null  object \n",
      " 4   Topic              55932 non-null  object \n",
      " 5   PublishDate        55932 non-null  object \n",
      " 6   Facebook           55932 non-null  int64  \n",
      " 7   GooglePlus         55932 non-null  int64  \n",
      " 8   LinkedIn           55932 non-null  int64  \n",
      " 9   SentimentTitle     55932 non-null  float64\n",
      " 10  SentimentHeadline  55932 non-null  float64\n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink               55932\n",
       "Title                48963\n",
       "Headline             52112\n",
       "Source                4753\n",
       "Topic                    4\n",
       "PublishDate          49602\n",
       "Facebook              2166\n",
       "GooglePlus             273\n",
       "LinkedIn               648\n",
       "SentimentTitle       10014\n",
       "SentimentHeadline    27265\n",
       "dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink                 0\n",
       "Title                  0\n",
       "Headline               0\n",
       "Source               175\n",
       "Topic                  0\n",
       "PublishDate            0\n",
       "Facebook               0\n",
       "GooglePlus             0\n",
       "LinkedIn               0\n",
       "SentimentTitle         0\n",
       "SentimentHeadline      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDLink           0\n",
       "Title            0\n",
       "Headline         0\n",
       "Source         101\n",
       "Topic            0\n",
       "PublishDate      0\n",
       "Facebook         0\n",
       "GooglePlus       0\n",
       "LinkedIn         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bloomberg         992\n",
       "Reuters           763\n",
       "ABC News          645\n",
       "New York Times    573\n",
       "The Guardian      551\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Source'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bloomberg          740\n",
       "Reuters            558\n",
       "ABC News           453\n",
       "New York Times     419\n",
       "MSPoweruser.com    416\n",
       "Name: Source, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Source'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Source'] = train_df['Source'].fillna('Bloomberg')\n",
    "test_df['Source'] = test_df['Source'].fillna('Bloomberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sanskar\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Sanskar\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sanskar\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "  text_token = word_tokenize(text)\n",
    "  filtered_text = ' '.join([w.lower() for w in text_token if w.lower() not in stop and len(w) > 2])\n",
    "  filtered_text = filtered_text.replace(r\"[^a-zA-Z]+\", '')\n",
    "  text_only = re.sub(r'\\b\\d+\\b', '', filtered_text)\n",
    "  clean_text = text_only.replace(',', '').replace('.', '').replace(':', '')\n",
    "  return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Text_Title'] = train_df['Title'] + ' ' + train_df['Source'] + ' ' + train_df['Topic']\n",
    "test_df['Text_Title'] = test_df['Title'] + ' ' + test_df['Source'] + ' ' + test_df['Topic']\n",
    "\n",
    "train_df['Text_Headline'] = train_df['Headline'] + ' ' + train_df['Source'] + ' ' + train_df['Topic']\n",
    "test_df['Text_Headline'] = test_df['Headline'] + ' ' + test_df['Source'] + ' ' + test_df['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tourism, govt spending buoys Thai economy in January The Nation - Thailand&#39;s English news economy'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Text_Title'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Text_Title'] = [clean(x) for x in train_df['Text_Title']]\n",
    "test_df['Text_Title'] = [clean(x) for x in test_df['Text_Title']]\n",
    "\n",
    "train_df['Text_Headline'] = [clean(x) for x in train_df['Text_Headline']]\n",
    "test_df['Text_Headline'] = [clean(x) for x in test_df['Text_Headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tourism govt spending buoys thai economy january nation thailand english news economy'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Text_Title'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "train_v_Title = vectorizer.fit_transform(train_df['Text_Title'])\n",
    "test_v_Title = vectorizer.transform(test_df['Text_Title'])\n",
    "\n",
    "vectorizer_ = TfidfVectorizer()\n",
    "\n",
    "train_v_Headline = vectorizer_.fit_transform(train_df['Text_Headline'])\n",
    "test_v_Headline = vectorizer_.transform(test_df['Text_Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20)\n",
    "\n",
    "train_v_Title = svd.fit_transform(train_v_Title)\n",
    "test_v_Title = svd.transform(test_v_Title)\n",
    "\n",
    "train_v_Headline = svd.fit_transform(train_v_Headline)\n",
    "test_v_Headline = svd.transform(test_v_Headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 602)\t0.3994843401495838\n",
      "  (0, 576)\t0.542936416844237\n",
      "  (0, 926)\t0.5576218975612863\n",
      "  (0, 887)\t0.4844482723673839\n",
      "  (1, 506)\t0.5284223537642353\n",
      "  (1, 384)\t0.5119178713761268\n",
      "  (1, 149)\t0.46609262810950425\n",
      "  (1, 244)\t0.320031564096322\n",
      "  (1, 94)\t0.37289055902776347\n",
      "  (2, 244)\t0.4129045437112839\n",
      "  (2, 94)\t0.48110318919437406\n",
      "  (2, 353)\t0.4856626508532062\n",
      "  (2, 60)\t0.6018150452554468\n",
      "  (3, 244)\t0.20043051470250942\n",
      "  (3, 342)\t0.6436248989412622\n",
      "  (3, 271)\t0.6800464215596224\n",
      "  (3, 588)\t0.28829058736436336\n",
      "  (4, 244)\t0.22601424249109275\n",
      "  (4, 588)\t0.16254455769167617\n",
      "  (4, 893)\t0.41721320344866597\n",
      "  (4, 363)\t0.40989879925258926\n",
      "  (4, 811)\t0.3624326124288049\n",
      "  (4, 461)\t0.40202980581305553\n",
      "  (4, 575)\t0.3572165646873905\n",
      "  (4, 255)\t0.3997798253967682\n",
      "  :\t:\n",
      "  (55926, 389)\t0.42476727554515387\n",
      "  (55927, 602)\t0.3488405199831052\n",
      "  (55927, 949)\t0.4027834187494174\n",
      "  (55927, 833)\t0.4071930559377812\n",
      "  (55927, 470)\t0.3862427984652355\n",
      "  (55927, 940)\t0.4385167027647823\n",
      "  (55927, 200)\t0.4569345975350807\n",
      "  (55928, 602)\t0.3380762243204932\n",
      "  (55928, 467)\t0.5655995940228308\n",
      "  (55928, 840)\t0.4772527118351688\n",
      "  (55928, 107)\t0.5814046911028375\n",
      "  (55929, 602)\t0.2888860465616721\n",
      "  (55929, 642)\t0.41249947529898673\n",
      "  (55929, 660)\t0.28396637376315637\n",
      "  (55929, 411)\t0.37308056694285047\n",
      "  (55929, 879)\t0.46637455037821773\n",
      "  (55929, 905)\t0.34062554295647035\n",
      "  (55929, 61)\t0.43935411955114034\n",
      "  (55930, 545)\t0.40945608945031087\n",
      "  (55930, 299)\t0.5731067695456054\n",
      "  (55930, 844)\t0.43553478510416005\n",
      "  (55930, 712)\t0.5605388411852763\n",
      "  (55931, 244)\t0.5235408621420443\n",
      "  (55931, 954)\t0.641408065596658\n",
      "  (55931, 660)\t0.5608035833115889\n"
     ]
    }
   ],
   "source": [
    "print(train_v_Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['polarity_t'] = train_df['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test_df['polarity_t'] = test_df['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "train_df['subjectivity_t'] = train_df['Title'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "test_df['subjectivity_t'] = test_df['Title'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['polarity_h'] = train_df['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test_df['polarity_h'] = test_df['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "train_df['subjectivity_h'] = train_df['Headline'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "test_df['subjectivity_h'] = test_df['Headline'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['Topic'] = encoder.fit_transform(train_df['Topic'])\n",
    "test_df['Topic'] = encoder.transform(test_df['Topic'])\n",
    "\n",
    "total = train_df['Source'].to_list() + test_df['Source'].to_list()\n",
    "total = encoder.fit_transform(total)\n",
    "train_df['Source'] = encoder.transform(train_df['Source'])\n",
    "test_df['Source'] = encoder.transform(test_df['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get day-type(monday, tuesday) from datetime\n",
    "\n",
    "train_weekday = []\n",
    "test_weekday = []\n",
    "\n",
    "for i in train_df['PublishDate']:\n",
    "    train_weekday.append(datetime.datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\").strftime(\"%A\"))\n",
    "    \n",
    "for i in test_df['PublishDate']:\n",
    "    test_weekday.append(datetime.datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\").strftime(\"%A\"))\n",
    "\n",
    "train_df['weekday'] = train_weekday\n",
    "test_df['weekday'] = test_weekday\n",
    "\n",
    "\n",
    "# convert weekday to 0-6\n",
    "\n",
    "train_df['weekday'] = train_df['weekday'].map({'Monday': 0,\n",
    "                                        'Tuesday': 1,\n",
    "                                        'Wednesday': 2,\n",
    "                                        'Thursday': 3,\n",
    "                                        'Friday': 4,\n",
    "                                        'Saturday': 5,\n",
    "                                        'Sunday': 6})\n",
    "test_df['weekday'] = test_df['weekday'].map({'Monday': 0,\n",
    "                                        'Tuesday': 1,\n",
    "                                        'Wednesday': 2,\n",
    "                                        'Thursday': 3,\n",
    "                                        'Friday': 4,\n",
    "                                        'Saturday': 5,\n",
    "                                        'Sunday': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour from date\n",
    "\n",
    "train_df[\"hour\"] = train_df[\"PublishDate\"].apply(lambda x: x.split()[1].split(':')[0])\n",
    "test_df[\"hour\"] = test_df[\"PublishDate\"].apply(lambda x: x.split()[1].split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in the Title \n",
    "train_df[\"num_words_t\"] = train_df[\"Text_Title\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words_t\"] = test_df[\"Text_Title\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Number of unique words in the Title \n",
    "train_df[\"num_unique_words_t\"] = train_df[\"Text_Title\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words_t\"] = test_df[\"Text_Title\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Number of characters in the Title \n",
    "train_df[\"num_chars_t\"] = train_df[\"Text_Title\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars_t\"] = test_df[\"Text_Title\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Average length of the words in the Title \n",
    "train_df[\"mean_word_len_t\"] = train_df[\"Text_Title\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len_t\"] = test_df[\"Text_Title\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of words in the Headline \n",
    "train_df[\"num_words_h\"] = train_df[\"Text_Headline\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words_h\"] = test_df[\"Text_Headline\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Number of unique words in the Headline \n",
    "train_df[\"num_unique_words_h\"] = train_df[\"Text_Headline\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words_h\"] = test_df[\"Text_Headline\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Number of characters in the Headline \n",
    "train_df[\"num_chars_h\"] = train_df[\"Text_Headline\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars_h\"] = test_df[\"Text_Headline\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Average length of the words in the Headline \n",
    "train_df[\"mean_word_len_h\"] = train_df[\"Text_Headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len_h\"] = test_df[\"Text_Headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "cols = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn', 'num_words_t', 'num_unique_words_t', 'num_chars_t', 'mean_word_len_t',\n",
    "        'num_words_h', 'num_unique_words_h', 'num_chars_h', 'mean_word_len_h', 'hour', 'weekday']\n",
    "\n",
    "for col in cols:\n",
    "    train_df[col] = scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n",
    "    test_df[col] = scaler.transform(test_df[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_t = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn', 'num_words_t', 'num_unique_words_t', 'mean_word_len_t','num_chars_t', 'polarity_t', 'subjectivity_t', 'hour', 'weekday']\n",
    "train_X1 = train[cols_t]\n",
    "test_X1 = test[cols_t]\n",
    "\n",
    "cols_h = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn', 'num_words_t', 'num_unique_words_t', 'mean_word_len_t','num_chars_t', 'polarity_t', 'subjectivity_t','num_words_h', 'num_unique_words_h', 'mean_word_len_h','num_chars_h', 'polarity_h', 'subjectivity_h', 'hour', 'weekday']\n",
    "train_X2 = train[cols_h]\n",
    "test_X2 = test[cols_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>num_words_t</th>\n",
       "      <th>num_unique_words_t</th>\n",
       "      <th>mean_word_len_t</th>\n",
       "      <th>num_chars_t</th>\n",
       "      <th>polarity_t</th>\n",
       "      <th>subjectivity_t</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.257700</td>\n",
       "      <td>0.841443</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-0.259144</td>\n",
       "      <td>-0.304659</td>\n",
       "      <td>-0.612899</td>\n",
       "      <td>-0.572668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>-0.795924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.314091</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.668699</td>\n",
       "      <td>-1.721681</td>\n",
       "      <td>0.274497</td>\n",
       "      <td>-1.629548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.338369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.314091</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.198848</td>\n",
       "      <td>-1.249340</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>-1.035053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.338369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.468412</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.668699</td>\n",
       "      <td>-1.249340</td>\n",
       "      <td>-1.436911</td>\n",
       "      <td>-2.224043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.871942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.029071</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>1.150411</td>\n",
       "      <td>1.112364</td>\n",
       "      <td>-0.295972</td>\n",
       "      <td>1.012652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.871942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Source     Topic  Facebook  GooglePlus  LinkedIn  num_words_t  \\\n",
       "0  1.257700  0.841443 -0.184044   -0.262649 -0.199608    -0.259144   \n",
       "1 -1.314091 -1.108773 -0.184044   -0.262649 -0.199608    -1.668699   \n",
       "2 -1.314091 -1.108773 -0.184044   -0.262649 -0.199608    -1.198848   \n",
       "3  0.468412 -1.108773 -0.184044   -0.262649 -0.199608    -1.668699   \n",
       "4  1.029071 -1.108773 -0.184044   -0.262649 -0.199608     1.150411   \n",
       "\n",
       "   num_unique_words_t  mean_word_len_t  num_chars_t  polarity_t  \\\n",
       "0           -0.304659        -0.612899    -0.572668         0.0   \n",
       "1           -1.721681         0.274497    -1.629548         0.0   \n",
       "2           -1.249340         0.328828    -1.035053         0.0   \n",
       "3           -1.249340        -1.436911    -2.224043         0.0   \n",
       "4            1.112364        -0.295972     1.012652         0.0   \n",
       "\n",
       "   subjectivity_t      hour   weekday  \n",
       "0             0.0 -1.699073 -0.795924  \n",
       "1             0.0 -1.699073  1.338369  \n",
       "2             0.0 -1.699073  1.338369  \n",
       "3             0.0 -1.699073  1.871942  \n",
       "4             0.0 -1.699073  1.871942  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.2576997634688756\n",
      "  (0, 1)\t0.841443095428152\n",
      "  (0, 2)\t-0.1840444807723746\n",
      "  (0, 3)\t-0.2626493550487806\n",
      "  (0, 4)\t-0.1996084239198007\n",
      "  (0, 5)\t-0.259144234496245\n",
      "  (0, 6)\t-0.3046586928529652\n",
      "  (0, 7)\t-0.6128993994536583\n",
      "  (0, 8)\t-0.5726681178922972\n",
      "  (0, 11)\t-1.6990733189086988\n",
      "  (0, 12)\t-0.7959239487209263\n",
      "  (1, 0)\t-1.3140910230329217\n",
      "  (1, 1)\t-1.1087732192299813\n",
      "  (1, 2)\t-0.1840444807723746\n",
      "  (1, 3)\t-0.2626493550487806\n",
      "  (1, 4)\t-0.1996084239198007\n",
      "  (1, 5)\t-1.668699356916161\n",
      "  (1, 6)\t-1.7216813679265908\n",
      "  (1, 7)\t0.274497439940456\n",
      "  (1, 8)\t-1.6295483600819824\n",
      "  (1, 11)\t-1.6990733189086988\n",
      "  (1, 12)\t1.3383691151678876\n",
      "  (2, 0)\t-1.3140910230329217\n",
      "  (2, 1)\t-1.1087732192299813\n",
      "  (2, 2)\t-0.1840444807723746\n",
      "  :\t:\n",
      "  (55929, 11)\t-1.5553969654148538\n",
      "  (55929, 12)\t-0.7959239487209263\n",
      "  (55930, 0)\t0.7923992191163056\n",
      "  (55930, 1)\t-0.13366506190091465\n",
      "  (55930, 2)\t-0.18266121113151082\n",
      "  (55930, 3)\t-0.2153389244819818\n",
      "  (55930, 4)\t-0.1865622345169816\n",
      "  (55930, 5)\t-1.1988476494428557\n",
      "  (55930, 6)\t-1.2493404762353821\n",
      "  (55930, 7)\t1.1437841397550987\n",
      "  (55930, 8)\t-0.7708331633028632\n",
      "  (55930, 10)\t1.0\n",
      "  (55930, 11)\t-1.5553969654148538\n",
      "  (55930, 12)\t-0.7959239487209263\n",
      "  (55931, 0)\t1.4800100235484368\n",
      "  (55931, 1)\t-1.1087732192299813\n",
      "  (55931, 2)\t-0.07891598806672769\n",
      "  (55931, 3)\t0.1158340894856098\n",
      "  (55931, 4)\t0.06131536413658131\n",
      "  (55931, 5)\t-1.668699356916161\n",
      "  (55931, 6)\t-1.7216813679265908\n",
      "  (55931, 7)\t0.274497439940456\n",
      "  (55931, 8)\t-1.6295483600819824\n",
      "  (55931, 11)\t-1.5553969654148538\n",
      "  (55931, 12)\t-0.7959239487209263\n"
     ]
    }
   ],
   "source": [
    "print(csr_matrix(train_X1.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>num_words_t</th>\n",
       "      <th>num_unique_words_t</th>\n",
       "      <th>mean_word_len_t</th>\n",
       "      <th>num_chars_t</th>\n",
       "      <th>polarity_t</th>\n",
       "      <th>subjectivity_t</th>\n",
       "      <th>num_words_h</th>\n",
       "      <th>num_unique_words_h</th>\n",
       "      <th>mean_word_len_h</th>\n",
       "      <th>num_chars_h</th>\n",
       "      <th>polarity_h</th>\n",
       "      <th>subjectivity_h</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.257700</td>\n",
       "      <td>0.841443</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-0.259144</td>\n",
       "      <td>-0.304659</td>\n",
       "      <td>-0.612899</td>\n",
       "      <td>-0.572668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.285943</td>\n",
       "      <td>-0.526753</td>\n",
       "      <td>-1.058531</td>\n",
       "      <td>-0.490533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>-0.795924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.314091</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.668699</td>\n",
       "      <td>-1.721681</td>\n",
       "      <td>0.274497</td>\n",
       "      <td>-1.629548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.285943</td>\n",
       "      <td>-0.232503</td>\n",
       "      <td>-0.175891</td>\n",
       "      <td>-0.317671</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.338369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.314091</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.198848</td>\n",
       "      <td>-1.249340</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>-1.035053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.413305</td>\n",
       "      <td>-0.673878</td>\n",
       "      <td>0.618485</td>\n",
       "      <td>-0.300385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.338369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.468412</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>-1.668699</td>\n",
       "      <td>-1.249340</td>\n",
       "      <td>-1.436911</td>\n",
       "      <td>-2.224043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096143</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>0.843558</td>\n",
       "      <td>0.304632</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.871942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.029071</td>\n",
       "      <td>-1.108773</td>\n",
       "      <td>-0.184044</td>\n",
       "      <td>-0.262649</td>\n",
       "      <td>-0.199608</td>\n",
       "      <td>1.150411</td>\n",
       "      <td>1.112364</td>\n",
       "      <td>-0.295972</td>\n",
       "      <td>1.012652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223505</td>\n",
       "      <td>0.208872</td>\n",
       "      <td>0.618485</td>\n",
       "      <td>0.391063</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.380556</td>\n",
       "      <td>-1.699073</td>\n",
       "      <td>1.871942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Source     Topic  Facebook  GooglePlus  LinkedIn  num_words_t  \\\n",
       "0  1.257700  0.841443 -0.184044   -0.262649 -0.199608    -0.259144   \n",
       "1 -1.314091 -1.108773 -0.184044   -0.262649 -0.199608    -1.668699   \n",
       "2 -1.314091 -1.108773 -0.184044   -0.262649 -0.199608    -1.198848   \n",
       "3  0.468412 -1.108773 -0.184044   -0.262649 -0.199608    -1.668699   \n",
       "4  1.029071 -1.108773 -0.184044   -0.262649 -0.199608     1.150411   \n",
       "\n",
       "   num_unique_words_t  mean_word_len_t  num_chars_t  polarity_t  \\\n",
       "0           -0.304659        -0.612899    -0.572668         0.0   \n",
       "1           -1.721681         0.274497    -1.629548         0.0   \n",
       "2           -1.249340         0.328828    -1.035053         0.0   \n",
       "3           -1.249340        -1.436911    -2.224043         0.0   \n",
       "4            1.112364        -0.295972     1.012652         0.0   \n",
       "\n",
       "   subjectivity_t  num_words_h  num_unique_words_h  mean_word_len_h  \\\n",
       "0             0.0    -0.285943           -0.526753        -1.058531   \n",
       "1             0.0    -0.285943           -0.232503        -0.175891   \n",
       "2             0.0    -0.413305           -0.673878         0.618485   \n",
       "3             0.0     0.096143            0.061747         0.843558   \n",
       "4             0.0     0.223505            0.208872         0.618485   \n",
       "\n",
       "   num_chars_h  polarity_h  subjectivity_h      hour   weekday  \n",
       "0    -0.490533    0.000000        0.000000 -1.699073 -0.795924  \n",
       "1    -0.317671    0.100000        0.200000 -1.699073  1.338369  \n",
       "2    -0.300385    0.000000        0.041667 -1.699073  1.338369  \n",
       "3     0.304632   -0.166667        0.166667 -1.699073  1.871942  \n",
       "4     0.391063    0.133333        0.380556 -1.699073  1.871942  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55932, 13)\n",
      "(37288, 13)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_X1))\n",
    "print(np.shape(test_X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55932, 19)\n",
      "(37288, 19)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.shape(train_X2))\n",
    "print(np.shape(test_X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55932, 30)\n",
      "(37288, 30)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_v_Title))\n",
    "print(np.shape(test_v_Title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55932, 30)\n",
      "(37288, 30)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_v_Headline))\n",
    "print(np.shape(test_v_Headline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_Title = hstack([train_v_Title, csr_matrix(train_X1.values)])\n",
    "test_X_Title = hstack([test_v_Title, csr_matrix(test_X1.values)])\n",
    "y1 = train['SentimentTitle']\n",
    "\n",
    "train_X_Headline = hstack([train_v_Headline, csr_matrix(train_X2.values)])\n",
    "test_X_Headline = hstack([test_v_Headline, csr_matrix(test_X2.values)])\n",
    "y2 = train['SentimentHeadline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55932, 33)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_X_Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9067386758064184\n"
     ]
    }
   ],
   "source": [
    "# LinearSVR model for SentimentTitle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=2)\n",
    "\n",
    "clf1 = LinearSVR(C=0.2,max_iter=10000)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "mae1 = mean_absolute_error(y_pred1, y_test)\n",
    "print('MAE:', 1 - mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.8952747622646386\n"
     ]
    }
   ],
   "source": [
    "# LinearSVR model for SentimentHeadline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Headline, y2, test_size=0.20, random_state=2)\n",
    "\n",
    "clf2 = LinearSVR(C=0.1,max_iter=10000)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "mae2 = mean_absolute_error(y_pred2, y_test)\n",
    "print('MAE:', 1 - mae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.8998603276813505\n"
     ]
    }
   ],
   "source": [
    "#using LinearSVR for prediction\n",
    "print('MAE:', 1 - ((0.4 * mae1) + (0.6 * mae2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9168106274808605\n"
     ]
    }
   ],
   "source": [
    "#using XGB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=42)\n",
    "params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, \n",
    "          'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n",
    "clf1 = XGBRegressor(random_state=2, **params)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "mae1 = mean_absolute_error(y_pred1, y_test)\n",
    "print('MAE:', 1 - mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9041842435752636\n"
     ]
    }
   ],
   "source": [
    "#using XGB\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Headline, y2, test_size=0.20, random_state=42)\n",
    "\n",
    "params = {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 4, \n",
    "          'n_estimators': 250, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n",
    "clf2 = XGBRegressor(random_state=2, **params)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "mae2 = mean_absolute_error(y_pred2, y_test)\n",
    "print('MAE:', 1 - mae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9228767995170419\n"
     ]
    }
   ],
   "source": [
    "#using lgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=42)\n",
    "\n",
    "params = {}\n",
    "params['learning_rate'] = 0.03\n",
    "#params['boosting_type'] = 'gbdt'\n",
    "params['metric'] = 'l2'\n",
    "params['sub_feature'] = 0.55\n",
    "params['num_leaves'] = 60\n",
    "params['min_data'] = 30\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "clf1 = lgb.train(params,d_train, 100)\n",
    "\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "mae1 = mean_absolute_error(y_pred1, y_test)\n",
    "print('MAE:', 1 - mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9067788485348552\n"
     ]
    }
   ],
   "source": [
    "#using lgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Headline, y2, test_size=0.20, random_state=42)\n",
    "\n",
    "params = {}\n",
    "params['learning_rate'] = 0.029\n",
    "#params['boosting_type'] = 'gbdt'\n",
    "params['metric'] = 'l2'\n",
    "params['sub_feature'] = 0.55\n",
    "params['num_leaves'] = 40\n",
    "params['min_data'] = 50\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label = y_train)\n",
    "\n",
    "clf2 = lgb.train(params,d_train, 100)\n",
    "\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "mae2 = mean_absolute_error(y_pred2, y_test)\n",
    "print('MAE:', 1 - mae2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9219321112023146\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=2)\n",
    "\n",
    "clf1 = RandomForestRegressor(n_estimators =10)\n",
    "clf1.fit(X_train, y_train)\n",
    "    \n",
    "y_pred1 = clf1.predict(X_test)\n",
    "mae1 = mean_absolute_error(y_pred1, y_test)\n",
    "print('MAE:', 1 - mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.9072569868853468\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_Headline, y2, test_size=0.20, random_state=2)\n",
    "\n",
    "clf2 = RandomForestRegressor(n_estimators =10)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "mae2 = mean_absolute_error(y_pred2, y_test)\n",
    "print('MAE:', 1 - mae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LinearSVR for prediction as it gave the best results\n",
    "\n",
    "\n",
    "title = clf1.predict(test_X_Title)\n",
    "headline = clf2.predict(test_X_Headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['IDLink'] = test_id\n",
    "df['SentimentTitle'] = title\n",
    "df['SentimentHeadline'] = headline\n",
    "df.to_csv('submit_1_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
